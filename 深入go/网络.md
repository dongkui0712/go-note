# 网络

Go 在网络编程方面提倡的做法是，每来一个连接就开一个 goroutine 去处理。非常的用户友好，不用学习一些反人类的网络编程模式，并且性能是有保障的。这些都得益于 Go 的网络模块的实现。

由于 goroutine 的实现非常轻量，很容易就可以开很多的 goroutine，这为每条连接分配一个 goroutine 打好了基础。Go 对网络的处理，在用户层是阻塞的，实现层是非阻塞的。

## 非阻塞 io
Go 提供的网络接口，在用户层是阻塞的，这样最符合人们的编程习惯。在 runtime 层面，是用** epoll/kqueue **实现的非阻塞 io，为性能提供了保障。

## 如何实现

底层非阻塞io是如何实现的呢？

简单地说，所有文件描述符都被设置成非阻塞的，某个goroutine进行io操作，读或者写文件描述符，如果此刻io还没准备好，则这个goroutine会被放到系统的等待队列中，这个goroutine失去了运行权，但并不是真正的整个系统“阻塞”于系统调用。

后台还有一个poller会不停地进行poll，所有的文件描述符都被添加到了这个poller中的，当某个时刻一个文件描述符准备好了，poller就会唤醒之前因它而阻塞的goroutine，于是goroutine重新运行起来。

这个poller是在后台一直运行的，前面分析系统调度章节时为了简化并没有提起它。其实在proc.c文件中，runtime.main函数的第一行代码就是

```
newm(sysmon, nil);
```

这个意思就是新建一个M并让它运行sysmon函数，前面说过M就是机器的抽象，它会直接开一个物理线程。sysmon里面是个死循环，每睡眠一小会儿就会调用runtime.epoll函数，这个sysmon就是所谓的poller。

poller是一个比gc更高优先级的东西，何以见得呢？首先，垃圾回收只是用runtime.newproc建立出来的，它仅仅是个goroutine任务，而poller是直接用newm建立出来的，它跟startm是平级的。也就相当于gc只是线程池里的任务，而poller自身直接就是worker。然后，gc只是被触发性地发生的，是被动的。而poller却是每隔很短时间就会主动运行。

## 封装层次

从最原始的epoll系统调用，到提供给用户的网络库函数，可以分成三个封装层次。这三个层次分别是:

- 依赖于系统的api封装，
- 平台独立的runtime封装，
- 提供给用户的库的封装。

### 1.依赖于系统的api封装
最下面一层是依赖于系统部分的封装。各个平台下的实现并不一样，比如linux下是封装的epoll，freebsd下是封装的kqueue。以linux为例，实现了一组调用epoll相关系统调用的封装。

不管是哪个平台，最终都会将依赖于系统的部分封装好，提供一组函数供runtime使用。

### 2.平台独立的runtime封装
接下来是平台独立的poller的封装，也就是runtime层的封装。这一层封装是最复杂的，它对外提供的一组接口

### 3.提供给用户的库的封装
最后一层封装层次是提供给用户的net包。在net包中网络文件描述符都是用一个netFD结构体来表示的，其中有个成员就是pollDesc。

所有用户的net包的调用最终调用到pollDesc的上面那一组函数中，这样就实现了当goroutine读或写阻塞时会被放到等待队列。最终的效果就是用户层阻塞，底层非阻塞。

